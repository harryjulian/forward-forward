{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Callable, Sequence\n",
    "\n",
    "import jax\n",
    "from jax import value_and_grad, jit, vmap, Array\n",
    "from jax.random import KeyArray\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import chex\n",
    "from optax import adam, GradientTransformation, apply_updates\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "  sizes: Sequence[int]\n",
    "  seed: int\n",
    "  epochs: int\n",
    "  learning_rate: float\n",
    "  activation_fn: Callable\n",
    "  goodness_fn: Callable\n",
    "  flat_shape: Tuple[int] = (784,)\n",
    "\n",
    "class Layer(nn.Module):\n",
    "  size: int\n",
    "  activation_fn: Callable\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = x / jnp.linalg.norm(x, 2, keepdims = True)\n",
    "    return self.activation_fn(nn.Dense(self.size)(x))\n",
    "\n",
    "def create_network(sizes: Sequence[int], learning_rate: float, activation_fn: Callable):  \n",
    "  return [(Layer(size, activation_fn), adam(learning_rate)) for size in sizes]\n",
    "\n",
    "ForwardForwardLayer = Tuple[Layer, GradientTransformation]\n",
    "Network = List[ForwardForwardLayer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "def load() -> Tuple[Array, Array, Array, Array]:\n",
    "  \"\"\"Remotely load MNIST data to JAX Arrays.\"\"\"\n",
    "\n",
    "  # Load Data\n",
    "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "  # Scale & Flatten images\n",
    "  X_train = (X_train.astype(\"float32\") / 255).reshape(*X_train.shape[:-2], -1)\n",
    "  X_test = (X_test.astype(\"float32\") / 255).reshape(*X_test.shape[:-2], -1)\n",
    "\n",
    "  return (\n",
    "    jnp.array(X_train),\n",
    "    jnp.array(y_train),\n",
    "    jnp.array(X_test),\n",
    "    jnp.array(y_test)\n",
    "  )\n",
    "\n",
    "def swap_perfect(key: KeyArray, y: Array) -> jnp.array:\n",
    "  \"\"\"Swap labels such that each entry is definitely different to it's initial entry.\n",
    "  \n",
    "  Args:\n",
    "    key: jax.random.PRNGKey\n",
    "    y: jnp.array\n",
    "  \n",
    "  Returns:\n",
    "    y_out: jnp.array, of shuffled labels\n",
    "  \"\"\"\n",
    "\n",
    "  def swap(args):\n",
    "    key, label, uniques = (args)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    sample = jax.random.choice(subkey, jnp.setdiff1d(uniques, label))\n",
    "    return subkey, sample\n",
    "\n",
    "  uniques = jnp.unique(y, size = 10)\n",
    "\n",
    "  y_out = []\n",
    "  for label in y.flatten():\n",
    "    key, negative_label = swap((key, label, uniques))\n",
    "    y_out.append(negative_label)\n",
    "\n",
    "  return jnp.array(y_out).reshape(y.shape)\n",
    "\n",
    "@jax.jit\n",
    "def overlay(X: Array, y: Array, l: int = 25) -> Array:\n",
    "  \"\"\"Combines X and y into a single vector which is compatible with training\n",
    "  via the forward-forward algorithm. In this example, we make the top line of pixels\n",
    "  correspond to the given label. Also flattens each array for use with MLP.\n",
    "  \n",
    "  Args:\n",
    "    X: Training examples.\n",
    "    y: Correct or incorrect labels.\n",
    "  \n",
    "  Returns:\n",
    "    out -> Xy array.\n",
    "  \"\"\"\n",
    "  _X = X\n",
    "  return _X.at[:, 0:l].set(jnp.full((l, y.shape[0]), y).T)\n",
    "\n",
    "@jax.jit\n",
    "def prep_input(key: KeyArray, X: chex.Array, y: chex.Array) -> Tuple[chex.Array, chex.Array]:\n",
    "  X_pos = overlay(X, y)\n",
    "  X_neg = overlay(X, jax.random.permutation(key, y))  \n",
    "  return X_pos, X_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def goodness_original(a: Array) -> Array:\n",
    "  \"\"\"Goodness of fit as the sum of squares of activations.\"\"\"\n",
    "  return (a ** 2).sum()\n",
    "\n",
    "@jit\n",
    "def loss(\n",
    "  A_pos: Array, \n",
    "  A_neg: Array,\n",
    "  theta: float\n",
    ") -> float:\n",
    "  \"\"\"Compute loss on positive and negative examples.\"\"\"\n",
    "  loss_pos = ((goodness_original(A_pos) - theta) * -1)\n",
    "  loss_neg = (goodness_original(A_neg) - theta)\n",
    "  return (loss_pos + loss_neg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load()\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IndexUpdateRef(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), (slice(None, None, None), slice(0, 25, None)))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.at[:, 0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[5., 5., 5., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [4., 4., 4., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 5., 5., ..., 0., 0., 0.],\n",
       "       [6., 6., 6., ..., 0., 0., 0.],\n",
       "       [8., 8., 8., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chex.Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.train_state import TrainState\n",
    "from functools import partial\n",
    "\n",
    "def train_layer(\n",
    "  key: KeyArray,\n",
    "  X: chex.Array,\n",
    "  y: chex.Array,\n",
    "  fflayer: ForwardForwardLayer,\n",
    "  epochs: int,\n",
    "  theta: int, \n",
    "  flat_shape: Tuple[int] = (784,)\n",
    "):\n",
    "\n",
    "  @value_and_grad\n",
    "  @partial(jit, static_argnums=(3,))\n",
    "  def loss(params, X_pos, X_neg, goodness_fn):\n",
    "    A_pos = state.apply_fn({'params': params}, X_pos)\n",
    "    A_neg = state.apply_fn({'params': params}, X_neg)\n",
    "    loss_pos = -(goodness_fn(A_pos) - theta)\n",
    "    loss_neg = (goodness_fn(A_neg) - theta)\n",
    "    return (loss_pos + loss_neg).mean()\n",
    "\n",
    "  @jit\n",
    "  def train_step(inkey, X_pos, X_neg, state):\n",
    "    inkey, subkey = jax.random.split(inkey, 2)\n",
    "    loss_val, grads = loss(state.params, X_pos, X_neg)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return subkey, loss_val, state\n",
    "\n",
    "  X_init = jax.random.normal(key, flat_shape)\n",
    "  layer, optimizer = fflayer\n",
    "  params = layer.init(key, X_init)\n",
    "\n",
    "  state = TrainState.create(\n",
    "        apply_fn = layer.apply,\n",
    "        tx = optimizer,\n",
    "        params = params['params']\n",
    "    )\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    X_pos, X_neg = prep_input(subkey, X, y)\n",
    "    key, loss_val, state = train_step(subkey, X_pos, X_neg, state)\n",
    "    if epoch % 10 == 0: print(f'Epoch {epoch}, loss: {loss_val}')\n",
    "\n",
    "  # Get out to feed to next layer\n",
    "  X_in, _ = prep_input(subkey, X, y)\n",
    "  X_out = state.apply_fn({'params': state.params}, X_in)\n",
    "\n",
    "  return state, X_out\n",
    "\n",
    "TrainedNet = List[TrainState]\n",
    "\n",
    "def train(key: KeyArray, net: Network, X: chex.Array, y: chex.Array, epochs: int, theta: int) -> TrainedNet:\n",
    "  _X = X\n",
    "  trained = []\n",
    "\n",
    "  # Train all Network Layers\n",
    "  for l in net:\n",
    "    state, _X = train_layer(key, _X, y, l, epochs, theta)\n",
    "    trained.append(state)\n",
    "  \n",
    "  return trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "  trainedNet: TrainedNet,\n",
    "  X: Array,\n",
    "  y: Array,\n",
    "):\n",
    "  layer_activations = []\n",
    "  for label in jnp.unique(y):\n",
    "    y_sgl = jnp.full(y.shape, label)\n",
    "    X_t = overlay(X, y_sgl)\n",
    "\n",
    "    activations = []\n",
    "    for state in trainedNet:\n",
    "      A_t = state.apply_fn({'params': state.params}, X_t)\n",
    "      print(type(A_t))\n",
    "      activations.append(A_t)\n",
    "    \n",
    "    layer_activations.append(activations)\n",
    "  \n",
    "  #return jnp.argmax(jnp.concatenate(preds), axis = 1)\n",
    "  return layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n"
     ]
    }
   ],
   "source": [
    "activations = predict(out, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.array([i[0] for i in activations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n",
      "[0.02560646 0.02708386 0.02655082 ... 0.02623161 0.02462271 0.02641665]\n"
     ]
    }
   ],
   "source": [
    "for lab in activations:\n",
    "  overall = []\n",
    "  for i in lab:\n",
    "    overall.append(jnp.sum(i, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.01477796, -0.01367337, -0.01355522, ...,  0.02623161,\n",
       "        0.02462271,  0.02641665], dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[0][0].shape.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.213707\n",
      "111.53833\n",
      "124.179\n",
      "125.82974\n",
      "125.49598\n",
      "124.83792\n",
      "124.19403\n",
      "123.63144\n",
      "123.15091\n",
      "122.74477\n"
     ]
    }
   ],
   "source": [
    "for arr in activations:\n",
    "  print(jnp.sum(jnp.concatenate([i.flatten() for i in arr])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "X_train, y_train, _, _ = load()\n",
    "net = create_network([784, 500], 0.001, jax.nn.gelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/api_util.py:165: SyntaxWarning: Jitted function has static_argnums=(3,), but only accepts 3 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: -1.1920928955078125e-05\n",
      "Epoch 0, loss: -5.960464477539062e-07\n"
     ]
    }
   ],
   "source": [
    "out = train(key, net, X_train, y_train, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.2968895e-04  5.9063575e-04 -5.5327767e-04 ... -4.3996162e-04\n",
      "   4.0091891e-04 -4.5514942e-04]\n",
      " [-7.4209651e-04  4.6982433e-04 -6.4880645e-04 ... -1.1954390e-04\n",
      "   7.6915481e-04 -6.3632877e-05]\n",
      " [-5.3363788e-04  4.9608975e-04 -3.2628627e-04 ... -5.1652413e-04\n",
      "   4.8414589e-04 -3.3191088e-04]\n",
      " ...\n",
      " [-1.7386554e-04  6.4366229e-04 -4.5608421e-04 ... -7.8958255e-04\n",
      "   8.2040555e-04 -3.2022904e-04]\n",
      " [-6.9002621e-04  7.4529066e-04 -4.4723458e-04 ... -3.5964299e-04\n",
      "   6.8084250e-04 -2.2933687e-04]\n",
      " [-4.0271605e-04  5.5800460e-04 -3.7934104e-04 ... -5.6520308e-04\n",
      "   8.3004928e-04 -3.6683548e-04]]\n",
      "[[-4.7052230e-04  6.0838094e-04 -5.2044558e-04 ...  3.1496963e-04\n",
      "   2.7054030e-04 -2.6022096e-04]\n",
      " [-1.5891490e-04  1.8861234e-04 -5.9090240e-04 ...  3.7103155e-04\n",
      "   1.4261724e-04 -2.7947058e-04]\n",
      " [-4.7179338e-04  6.1404379e-04 -5.5733329e-04 ...  4.2096520e-04\n",
      "   5.4968859e-04 -5.5224111e-04]\n",
      " ...\n",
      " [-1.6870211e-04  7.4912957e-04 -5.6288246e-04 ...  2.7740659e-04\n",
      "   6.4104080e-04 -2.3615651e-04]\n",
      " [-1.9830740e-04  5.4603891e-04 -6.1201869e-04 ...  5.5712077e-04\n",
      "   4.3626150e-04 -2.8100441e-04]\n",
      " [ 1.9151632e-05  4.4268442e-04 -4.1333586e-04 ...  1.7252428e-04\n",
      "   5.4707489e-04 -3.2239337e-04]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum requires ndarray or scalar arguments, got <class 'list'> at position 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/HarryJulian/forward-forward/new.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/HarryJulian/forward-forward/new.ipynb#ch0000021?line=0'>1</a>\u001b[0m predict(out, X_test, y_test)\n",
      "\u001b[1;32m/Users/HarryJulian/forward-forward/new.ipynb Cell 8'\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(trainedNet, X, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HarryJulian/forward-forward/new.ipynb#ch0000017?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(A_t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HarryJulian/forward-forward/new.ipynb#ch0000017?line=14'>15</a>\u001b[0m     activations\u001b[39m.\u001b[39mappend(A_t)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/HarryJulian/forward-forward/new.ipynb#ch0000017?line=16'>17</a>\u001b[0m   preds\u001b[39m.\u001b[39mappend(jnp\u001b[39m.\u001b[39;49msum(activations))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HarryJulian/forward-forward/new.ipynb#ch0000017?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39margmax(jnp\u001b[39m.\u001b[39mconcatenate(preds), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py:215\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where, promote_integers)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=210'>211</a>\u001b[0m \u001b[39m@_wraps\u001b[39m(np\u001b[39m.\u001b[39msum, skip_params\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m], extra_params\u001b[39m=\u001b[39m_PROMOTE_INTEGERS_DOC)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=211'>212</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a: ArrayLike, axis: Axis \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dtype: DTypeLike \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=212'>213</a>\u001b[0m         out: \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, keepdims: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, initial: Optional[ArrayLike] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=213'>214</a>\u001b[0m         where: Optional[ArrayLike] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, promote_integers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=214'>215</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _reduce_sum(a, axis\u001b[39m=\u001b[39;49m_ensure_optional_axes(axis), dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=215'>216</a>\u001b[0m                      keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=216'>217</a>\u001b[0m                      promote_integers\u001b[39m=\u001b[39;49mpromote_integers)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py:205\u001b[0m, in \u001b[0;36m_reduce_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where, promote_integers)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=199'>200</a>\u001b[0m \u001b[39m@partial\u001b[39m(api\u001b[39m.\u001b[39mjit, static_argnames\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39maxis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpromote_integers\u001b[39m\u001b[39m'\u001b[39m), inline\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=200'>201</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_reduce_sum\u001b[39m(a: ArrayLike, axis: Axis \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dtype: DTypeLike \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=201'>202</a>\u001b[0m                 out: \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, keepdims: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=202'>203</a>\u001b[0m                 initial: Optional[ArrayLike] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, where: Optional[ArrayLike] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=203'>204</a>\u001b[0m                 promote_integers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=204'>205</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _reduction(a, \u001b[39m\"\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49msum, lax\u001b[39m.\u001b[39;49madd, \u001b[39m0\u001b[39;49m, preproc\u001b[39m=\u001b[39;49m_cast_to_numeric,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=205'>206</a>\u001b[0m                     bool_op\u001b[39m=\u001b[39;49mlax\u001b[39m.\u001b[39;49mbitwise_or, upcast_f16_for_computation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=206'>207</a>\u001b[0m                     axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=207'>208</a>\u001b[0m                     initial\u001b[39m=\u001b[39;49minitial, where_\u001b[39m=\u001b[39;49mwhere, parallel_reduce\u001b[39m=\u001b[39;49mlax\u001b[39m.\u001b[39;49mpsum,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=208'>209</a>\u001b[0m                     promote_integers\u001b[39m=\u001b[39;49mpromote_integers)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py:83\u001b[0m, in \u001b[0;36m_reduction\u001b[0;34m(a, name, np_fun, op, init_val, has_identity, preproc, bool_op, upcast_f16_for_computation, axis, dtype, out, keepdims, initial, where_, parallel_reduce, promote_integers)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=81'>82</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument to jnp.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=82'>83</a>\u001b[0m _check_arraylike(name, a)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=83'>84</a>\u001b[0m lax_internal\u001b[39m.\u001b[39m_check_user_dtype_supported(dtype, name)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/reductions.py?line=84'>85</a>\u001b[0m axis \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mconcrete_or_error(\u001b[39mNone\u001b[39;00m, axis, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maxis argument to jnp.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m().\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/util.py:345\u001b[0m, in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/util.py?line=341'>342</a>\u001b[0m pos, arg \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m((i, arg) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/util.py?line=342'>343</a>\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _arraylike(arg))\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/util.py?line=343'>344</a>\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m requires ndarray or scalar arguments, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m at position \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jax/_src/numpy/util.py?line=344'>345</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(fun_name, \u001b[39mtype\u001b[39m(arg), pos))\n",
      "\u001b[0;31mTypeError\u001b[0m: sum requires ndarray or scalar arguments, got <class 'list'> at position 0."
     ]
    }
   ],
   "source": [
    "predict(out, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.3'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flax\n",
    "flax.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.5'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chex\n",
    "chex.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
